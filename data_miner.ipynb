{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "labels = []\n",
    "titles = []\n",
    "desc = []\n",
    "page_no = 1\n",
    "list_url = 'https://github.com/twbs/bootstrap/issues?q=is%3Aissue'\n",
    "list_content = urlopen(list_url).read()\n",
    "list_soup = BeautifulSoup(list_content, 'html.parser')\n",
    "url_list = list_soup.find_all(class_='lh-condensed')\n",
    "for x in url_list:\n",
    "    labels_tag = x.find(class_='labels')\n",
    "    if labels_tag is not None:\n",
    "        urls.append('https://github.com' + x.find(class_='link-gray-dark')['href'])\n",
    "        labels.append([y.string for y in labels_tag.find_all(class_='label')])\n",
    "# print(urls, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while list_soup.find(class_='next_page disabled') is None:\n",
    "    page_no += 1\n",
    "    list_url = 'https://github.com/twbs/bootstrap/issues?q=is%3Aissue+&&page=' + str(page_no)\n",
    "    list_content = urlopen(list_url).read()\n",
    "    list_soup = BeautifulSoup(list_content, 'html.parser')\n",
    "    url_list = list_soup.find_all(class_='lh-condensed')\n",
    "    for x in url_list:\n",
    "        labels_tag = x.find(class_='labels')\n",
    "        if labels_tag is not None:\n",
    "            urls.append('https://github.com' + x.find(class_='link-gray-dark')['href'])\n",
    "            labels.append([y.string for y in labels_tag.find_all(class_='label')])\n",
    "    print(page_no, end='\\r')\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls_file = open('urls.json', 'w+')\n",
    "json.dump(urls, urls_file)\n",
    "urls_file.close()\n",
    "labels_file = open('labels.json', 'w+')\n",
    "json.dump(labels, labels_file)\n",
    "labels_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9501\n",
      "9501\r"
     ]
    }
   ],
   "source": [
    "issue_no = 1\n",
    "print(len(urls))\n",
    "for x in urls:\n",
    "    content = urlopen(x).read()\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    title = soup.title.string\n",
    "    title = title[0 : title.find(' Â· Issue #')]\n",
    "    titles.append(title)\n",
    "    description = soup.find('td', class_='comment-body')\n",
    "    for c in description.find_all('code') : c.extract() \n",
    "    desc.append(description.text)\n",
    "    print(issue_no, end='\\r')\n",
    "    issue_no += 1\n",
    "    time.sleep(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_file = open('titles.json', 'w+')\n",
    "json.dump(titles, titles_file)\n",
    "titles_file.close()\n",
    "descs_file = open('descs.json', 'w+')\n",
    "json.dump(desc, descs_file)\n",
    "descs_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
